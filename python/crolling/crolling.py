from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
import time
import pandas as pd
import urllib.parse
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


# --- 1. 29CM í¬ë¡¤ë§ í•¨ìˆ˜ ---
def crawl_29cm(keyword, pages=1):
    print(f"\n--- 29CM í¬ë¡¤ë§ ì‹œì‘ (ê²€ìƒ‰ì–´: '{keyword}', {pages}í˜ì´ì§€) ---")
    encoded_keyword = urllib.parse.quote(keyword)
    results_29cm = []

    options = Options()
    options.add_argument("--start-maximized")
    options.add_argument(
        "User-Agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36")
    options.add_experimental_option("detach", True)
    options.add_experimental_option("excludeSwitches", ["enable-logging", "enable-automation"])

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    for page in range(1, pages + 1):
        url = f'https://shop.29cm.co.kr/search?keyword={encoded_keyword}&sort=RECOMMENDED&page={page}'
        driver.get(url)
        print(f"ğŸ”— 29CM í¬ë¡¤ë§ ì¤‘: {url}")
        time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

        # í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤ (Lazy Load)
        last_height = driver.execute_script("return document.body.scrollHeight")
        while True:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height

        # ìƒí’ˆ ë¡œë”© ëŒ€ê¸°
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, "li > div.mb-40.space-y-12"))
            )
        except:
            print(f"âŒ 29CM {page}í˜ì´ì§€ ìƒí’ˆ ë¡œë”© ì‹¤íŒ¨ ë˜ëŠ” ìš”ì†Œ ì—†ìŒ.")
            continue

        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')

        items = soup.select("li > div.mb-40.space-y-12")  # ë‹¨ì¼ ìƒí’ˆ ì»¨í…Œì´ë„ˆ

        print(f"âœ… 29CM {page}í˜ì´ì§€ ({len(items)}ê°œ ìƒí’ˆ ë°œê²¬)")

        for rank, item in enumerate(items, 1):
            try:
                brand_element = item.select_one("span.align-middle.text-s-bold")
                brand = brand_element.text.strip() if brand_element else "ë¸Œëœë“œ ì—†ìŒ"

                title_element = item.select_one("a.line-clamp-2.break-all.mb-6")
                title = title_element.text.strip() if title_element else "ìƒí’ˆëª… ì—†ìŒ"

                discount_element = item.select_one("p.text-l-bold.text-accent")
                discount = discount_element.text.strip() if discount_element else "í• ì¸ ì—†ìŒ"

                price_element = item.select_one(
                    "div.items-center.flex.gap-2.text-xxl-bold.text-primary > p:nth-child(2)")
                price = price_element.text.strip() if price_element else "ê°€ê²© ì •ë³´ ì—†ìŒ"

                product_link_element = item.select_one("a.line-clamp-2.break-all.mb-6")
                product_link = product_link_element['href'] if product_link_element else "ë§í¬ ì—†ìŒ"

                results_29cm.append({
                    "ì‡¼í•‘ëª°": "29CM",
                    "ë¸Œëœë“œ": brand,
                    "ìƒí’ˆëª…": title,
                    "ê°€ê²©": price,
                    "í• ì¸ìœ¨": discount,
                    "ë§í¬": product_link
                })

            except Exception as e:
                print(f"29CM ìƒí’ˆ ì¶”ì¶œ ì˜¤ë¥˜ ({rank}ë²ˆ ìƒí’ˆ): {e}")
                pass

    driver.quit()
    print(f"--- 29CM í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(results_29cm)}ê°œ ìƒí’ˆ ---")
    return pd.DataFrame(results_29cm)


# --- 2. ë¬´ì‹ ì‚¬ í¬ë¡¤ë§ í•¨ìˆ˜ (ìµœì¢… ì—…ë°ì´íŠ¸ëœ ì„ íƒì ì ìš©) ---
def crawl_musinsa(keyword, pages=1):
    print(f"\n--- ë¬´ì‹ ì‚¬ í¬ë¡¤ë§ ì‹œì‘ (ê²€ìƒ‰ì–´: '{keyword}', {pages}í˜ì´ì§€) ---")
    encoded_keyword = urllib.parse.quote(keyword)
    results_musinsa = []

    # WebDriver ì„¤ì •ì€ ìƒëµ (ì „ì²´ ì½”ë“œì—ì„œëŠ” í¬í•¨ë˜ì–´ ìˆìŒ)
    options = Options()
    options.add_argument("--start-maximized")
    options.add_argument(
        "User-Agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36")
    options.add_experimental_option("detach", True)
    options.add_experimental_option("excludeSwitches", ["enable-logging", "enable-automation"])
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    for page in range(1, pages + 1):
        url = f'https://www.musinsa.com/search/musinsa/goods?q={encoded_keyword}&listType=small&target=category&sortCode=pop&page={page}'
        driver.get(url)
        print(f"ğŸ”— ë¬´ì‹ ì‚¬ í¬ë¡¤ë§ ì¤‘: {url}")
        time.sleep(3)

        # ìŠ¤í¬ë¡¤ ë¡œì§ ìƒëµ (ì „ì²´ ì½”ë“œì—ì„œëŠ” í¬í•¨ë˜ì–´ ìˆìŒ)
        last_height = driver.execute_script("return document.body.scrollHeight")
        while True:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height

        # ìƒí’ˆ ë¡œë”© ëŒ€ê¸°
        try:
            # ì „ì²´ ìƒí’ˆ ì»¨í…Œì´ë„ˆ ëŒ€ê¸°
            WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located(
                    (By.CSS_SELECTOR, "div.UIStyleComponents__UIItemContainerStyle-sc-d36st-1"))
            )
        except Exception as e:
            print(f"âŒ ë¬´ì‹ ì‚¬ {page}í˜ì´ì§€ ìƒí’ˆ ë¡œë”© ì‹¤íŒ¨ ë˜ëŠ” ìš”ì†Œ ì—†ìŒ.")
            continue

        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')

        # â­ ìƒí’ˆ ëª©ë¡ ì„ íƒì (ìµœëŒ€í•œ ì•ˆì •ì ìœ¼ë¡œ) â­
        items = soup.select("div.UIStyleComponents__UIItemContainerStyle-sc-d36st-1")

        print(f"âœ… ë¬´ì‹ ì‚¬ {page}í˜ì´ì§€ ({len(items)}ê°œ ìƒí’ˆ ë°œê²¬)")

        for rank, item in enumerate(items, 1):
            try:
                # 1. ë¸Œëœë“œëª…: 'ìƒµìœ¼ë¡œ ì´ë™' <a> íƒœê·¸ ì•ˆì˜ <span>
                brand_element = item.select_one("a[aria-label$='ìƒµìœ¼ë¡œ ì´ë™'] > span")
                brand = brand_element.text.strip() if brand_element else "ë¸Œëœë“œ ì—†ìŒ"

                # 2. ìƒí’ˆëª… ë° ë§í¬: 'ìƒí’ˆìƒì„¸ë¡œ ì´ë™' <a> íƒœê·¸
                title_link_element = item.select_one("a[aria-label$='ìƒí’ˆìƒì„¸ë¡œ ì´ë™']")
                product_link = title_link_element['href'] if title_link_element else "ë§í¬ ì—†ìŒ"

                # ìƒí’ˆëª…ì€ ë§í¬ íƒœê·¸ ì•ˆì˜ <span>ì—ì„œ ì¶”ì¶œ
                title_element = title_link_element.select_one("span")
                title = title_element.text.strip() if title_element else "ìƒí’ˆëª… ì—†ìŒ"

                # 3. ê°€ê²© ë° í• ì¸ìœ¨
                # ê°€ê²© ì»¨í…Œì´ë„ˆ: sc-jwTyAe sc-hjsuWn DVxSk ANWFy
                price_container = item.select_one("div.sc-jwTyAe.sc-hjsuWn.DVxSk.ANWFy")

                discount = "í• ì¸ ì—†ìŒ"
                price = "ê°€ê²© ì •ë³´ ì—†ìŒ"

                if price_container:
                    # í• ì¸ìœ¨: text-red í´ë˜ìŠ¤ë¥¼ ê°€ì§„ span
                    discount_element = price_container.select_one("span.text-red")
                    discount = discount_element.text.strip() if discount_element else "í• ì¸ ì—†ìŒ"

                    # ê°€ê²©: text-redê°€ ì—†ëŠ” spanì„ ê°€ê²©ìœ¼ë¡œ ê°„ì£¼
                    # ë˜ëŠ” ëª¨ë“  span ì¤‘ ë§ˆì§€ë§‰ spanì„ ê°€ê²©ìœ¼ë¡œ ê°„ì£¼
                    price_element = price_container.select_one("span:not(.text-red)")
                    if price_element:
                        price = price_element.text.strip()
                    else:
                        # í• ì¸ ì—†ëŠ” ìƒí’ˆì˜ ê²½ìš° í•˜ë‚˜ì˜ spanë§Œ ìˆì„ ìˆ˜ ìˆìŒ (ê°€ê²©)
                        all_spans = price_container.select("span")
                        if all_spans and discount == "í• ì¸ ì—†ìŒ":
                            price = all_spans[-1].text.strip()

                results_musinsa.append({
                    "ì‡¼í•‘ëª°": "ë¬´ì‹ ì‚¬",
                    "ë¸Œëœë“œ": brand,
                    "ìƒí’ˆëª…": title,
                    "ê°€ê²©": price,
                    "í• ì¸ìœ¨": discount,
                    "ë§í¬": product_link
                })

            except Exception as e:
                # print(f"ë¬´ì‹ ì‚¬ ìƒí’ˆ ì¶”ì¶œ ì˜¤ë¥˜ ({rank}ë²ˆ ìƒí’ˆ): {e}")
                pass

    driver.quit()
    print(f"--- ë¬´ì‹ ì‚¬ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(results_musinsa)}ê°œ ìƒí’ˆ ---")
    return pd.DataFrame(results_musinsa)

# --- 3. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ ---
if __name__ == "__main__":
    search_keyword = input("í¬ë¡¤ë§í•  ì˜· ì¹´í…Œê³ ë¦¬/ìƒí’ˆì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: í›„ë“œí‹°, ì½”íŠ¸): ")
    num_pages = int(input("ê° ì‡¼í•‘ëª°ì—ì„œ ëª‡ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í• ê¹Œìš”? (ìˆ«ì ì…ë ¥): "))

    # 29CM í¬ë¡¤ë§ ì‹¤í–‰
    df_29cm = crawl_29cm(search_keyword, pages=num_pages)

    # ë¬´ì‹ ì‚¬ í¬ë¡¤ë§ ì‹¤í–‰
    df_musinsa = crawl_musinsa(search_keyword, pages=num_pages)

    # ê²°ê³¼ ë³‘í•© (ë‘ DataFrameì´ ìˆë‹¤ë©´)
    if not df_29cm.empty and not df_musinsa.empty:
        combined_df = pd.concat([df_29cm, df_musinsa], ignore_index=True)
        combined_df.to_csv(f"shopping_comparison_{search_keyword}.csv", index=False, encoding="utf-8-sig")
        print(f"\nâœ… 29CMì™€ ë¬´ì‹ ì‚¬ í¬ë¡¤ë§ ê²°ê³¼ê°€ 'shopping_comparison_{search_keyword}.csv' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    elif not df_29cm.empty:
        df_29cm.to_csv(f"29cm_{search_keyword}.csv", index=False, encoding="utf-8-sig")
        print(f"\nâœ… 29CM í¬ë¡¤ë§ ê²°ê³¼ë§Œ '29cm_{search_keyword}.csv' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    elif not df_musinsa.empty:
        df_musinsa.to_csv(f"musinsa_{search_keyword}.csv", index=False, encoding="utf-8-sig")
        print(f"\nâœ… ë¬´ì‹ ì‚¬ í¬ë¡¤ë§ ê²°ê³¼ë§Œ 'musinsa_{search_keyword}.csv' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ ë‘ ì‡¼í•‘ëª° ëª¨ë‘ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    print("\nëª¨ë“  í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")